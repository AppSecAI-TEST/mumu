JobConf.setNumMapTasks(n)是有意义的，结合block size会具体影响到map任务的个数，详见FileInputFormat.getSplits源码。
假设没有设置mapred.min.split.size，缺省为1的情况下，针对每个文件会按照min (totalsize[所有文件总大小]/mapnum[jobconf设置
的mapnum], blocksize)为大小来拆分，并不是说文件小于block size就不去拆分。

不知道你是要提高整个集群的map/reduce任务数，还是单个节点可并行运行的map/reduce任务数？对于前者是一般只设置reduce任务数
而map任务数是由Splits个数决定的; 对于后者，是可以在配置中设置的，
分别为：mapred.tasktracker.map.tasks.maximum 
        mapred.tasktracker.reduce.tasks.maximum 

另外，还有个参数mapred.jobtracker.taskScheduler.maxRunningTasksPerJob，
用来控制一个job最大并行tasks数，这个是指在集群最大并行数。

3.我的理解：具体看FileInputFormat.java的代码 
map tasks的个数只要是看splitSize，一个文件根据splitSize分成多少份就有多少个map tasks。而splitSize的计算(看FileInputFormat的源码)：
splitSize = Math.max(minSize, Math.min(maxSize, blockSize));
而minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job));
即是某种格式的文件的最小分割size(如看源码sequenceFile是2000)和整个job配置的最小分割size（即mapred-default.xml中mapred.min.split.size的值）
之间的较大的那个 
maxSize是mapred.max.split.size（mapred-default.xml中竟然没有，我试了一下，在mapred-site.xml中配置覆盖也没有用，
具体用法参照http://osdir.com/ml/mahout-user.lucene.apache.org/2010-01/msg00231.html
用参数配置： hadoop jar /root/mahout-core-0.2.job org.apache.mahout.clustering.lda.LDADriver -Dmapred.max.split.size=900...）,
如果不配置，默认值是long类型的最大值。（mapred.max.split.size不推荐配置（试）） 
blockSize是即hdfs-default.xml中dfs.block.size的值,可在hdf-site.xml中覆盖.这个值必须是512的倍数，如果想要数量更多的map的tasks的个数，
可以把dfs.block.size设得小一点，512，1024等等，反正上面的公式保证了即使你这个blocksize设得比某种格式的文件的最小分割size要小，
最后还是选者这种格式的最小分割size，如果blocksize比它大，则选用blocksize作为splitSize的大小. 

总结：如果想要多一点的map tasks，(1)可以设置dfs.block.size小一点，sequenceFile推荐2048。。。
（试）在eclipse运行时，dfs.block.size是由eclipse中mapreduce的设置（dfs.block.size）生效的，
而不是hadoop的conf中的配置文件，但是如果用终端hadoop jar命令跑的话，应该是由hadoop的conf中的配置文件决定生效的 
(2)推荐： 可以分成多个sequenceFile来作为输入（把上层目录作为输入路径即可，上层目录下包括的必为清一色的sequenceFile）,
输入路径 "./"或指定上层目录文件名 

reduce task的个数： 
可通过job.setNumReduceTasks(n);设定。多个reduce task的话就会有多个reduce结果，part-r-00000, part-r-00001, ...part-r-0000n 



增加task的数量，一方面增加了系统的开销，另一方面增加了负载平衡和减小了任务失败的代价；
map task的数量即mapred.map.tasks的参数值，用户不能直接设置这个参数。Input Split的大小，决定了一个Job拥有多少个map。
默认input split的大小是64M（与dfs.block.size的默认值相同）。然而，如果输入的数据量巨大，那么默认的64M的block会有几万甚至几十万的Map Task，
集群的网络传输会很大，最严重的是给Job Tracker的调度、队列、内存都会带来很大压力。
mapred.min.split.size这个配置项决定了每个 Input Split的最小值，用户可以修改这个参数，从而改变map task的数量。
一个恰当的map并行度是大约每个节点10-100个map，且最好每个map的执行时间至少一分钟。
 reduce task的数量由mapred.reduce.tasks这个参数设定，默认值是1。
合适的reduce task数量是0.95或者0.75*( nodes * mapred.tasktracker.reduce.tasks.maximum), 
其中，mapred.tasktracker.tasks.reduce.maximum的数量一般设置为各节点cpu core数量，即能同时计算的slot数量。
对于0.95，当map结束时，所有的reduce能够立即启动；对于1.75，较快的节点结束第一轮reduce后，可以开始第二轮的reduce任务，从而提高负载均衡




由Hive来执行相关的查询
Hadoop中默认的mapred.tasktracker.map.tasks.maximum设置是2
也即：每一个tasktracker同时运行的map任务数为2
照此默认设置，查询80天某用户的操作日志，耗时5mins, 45sec
经过测试，发现将mapred.tasktracker.map.tasks.maximum设置为节点的cpu cores数目或者数目减1比较合适
此时的运行效率最高，大概花费3mins, 25sec
我们现在的机器都是8核的，所以最终配置如下：
<property>
    <name>mapred.tasktracker.map.tasks.maximum</name>
    <value>8</value>
    <description>The maximum number of map tasks that will be run
    simultaneously by a task tracker.
    </description>
</property>
而对于mapred.map.tasks（每个job的map任务数）值，hadoop默认值也为2
可以在执行hive前，通过set mapred.map.tasks=24来设定
但由于使用hive，会操作多个input文件，所以hive默认会把map的任务数设置成输入的文件数目
即使你通过set设置了数目，也不起作用…







Hadoop自带了一个历史服务器，可以通过历史服务器查看已经运行完的Mapreduce作业记录，比如用了多少个Map、用了多少个Reduce、作业提交时间、作业启动时间、作业完成时间等信息。默认情况下，Hadoop历史服务器是没有启动的，我们可以通过下面的命令来启动Hadoop历史服务器

$ sbin/mr-jobhistory-daemon.sh  start historyserver

这样我们就可以在相应机器的19888端口上打开历史服务器的WEB UI界面。可以查看已经运行完的作业情况。历史服务器可以单独在一台机器上启动，主要是通过以下的参数配置：

<property>
    <name>mapreduce.jobhistory.address</name>
    <value>0.0.0.0:10020</value>
</property>

<property>
    <name>mapreduce.jobhistory.webapp.address</name>
    <value>0.0.0.0:19888</value>
</property>

上面的参数是在mapred-site.xml文件中进行配置，mapreduce.jobhistory.address和mapreduce.jobhistory.webapp.address默认的值分别是0.0.0.0:10020和0.0.0.0:19888，大家可以根据自己的情况进行相应的配置，参数的格式是host:port。配置完上述的参数之后，重新启动Hadoop jobhistory，这样我们就可以在mapreduce.jobhistory.webapp.address参数配置的主机上对Hadoop历史作业情况经行查看。

很多人就会问了，这些历史数据是存放在哪里的？是存放在HDFS中的，我们可以通过下面的配置来设置在HDFS的什么目录下存放历史作业记录：

<property>
    <name>mapreduce.jobhistory.done-dir</name>
    <value>${yarn.app.mapreduce.am.staging-dir}/history/done</value>
</property>

<property>
    <name>mapreduce.jobhistory.intermediate-done-dir</name>
    <value>${yarn.app.mapreduce.am.staging-dir}
                        /history/done_intermediate</value>
</property>

<property>
    <name>yarn.app.mapreduce.am.staging-dir</name>
    <value>/tmp/hadoop-yarn/staging</value>
</property>

上面的配置都默认的值，我们可以在mapred-site.xml文件中进行修改。其中，mapreduce.jobhistory.done-dir参数的意思是在什么目录下存放已经运行完的Hadoop作业记录；mapreduce.jobhistory.intermediate-done-dir的意思是正在运行的Hadoop作业记录。我们可以到mapreduce.jobhistory.done-dir参数配置的目录下看看里面存放的是什么东西：

[wyp@master /home/wyp/hadoop]# bin/hadoop fs -ls /jobs/done/
Found 2 items
drwxrwx---  - wyp supergroup          0 2013-12-03 23:36 /jobs/done/2013
drwxrwx---  - wyp supergroup          0 2014-02-01 00:02 /jobs/done/2014

[wyp@master /home/wyp/hadoop]# bin/hadoop fs -ls /jobs/done/2014/02/16
Found 27 items
drwxrwx--- - wyp supergroup 0 2014-02-16 02:02 /jobs/done/2014/02/16/001216
drwxrwx--- - wyp supergroup 0 2014-02-16 02:44 /jobs/done/2014/02/16/001217
drwxrwx--- - wyp supergroup 0 2014-02-16 03:38 /jobs/done/2014/02/16/001218
drwxrwx--- - wyp supergroup 0 2014-02-16 04:20 /jobs/done/2014/02/16/001219
drwxrwx--- - wyp supergroup 0 2014-02-16 05:14 /jobs/done/2014/02/16/001220

[wyp@master hadoop]# bin/hadoop fs -ls /jobs/done/2014/02/16/001216
Found 1318 items
-rwxrwx---  3 wyp    supergroup  45541335 2014-02-16 00:11 /jobs/done/2014
/02/16/001216/job_1388830974669_1216161-1392478837250-wyp-insert+overwrite
+table+qt_city_query_ana...e%28Stage-1392480689141-5894-33-SUCCEEDED-wyp.jhist
-rwxrwx---  3 wyp    supergroup    193572 2014-02-16 00:11 /jobs/done
/2014/02/16/001216/job_1388830974669_1216161_conf.xml
-rwxrwx---  3 wyp    supergroup  45594759 2014-02-16 00:11 /jobs/done/2014
/02/16/001216/job_1388830974669_1216162-1392478837250-wyp-insert+overwrite
+table+qt_city_query_ana...e%28Stage-1392480694818-5894-33-SUCCEEDED-wyp.jhist
-rwxrwx---  3 wyp    supergroup    193572 2014-02-16 00:11 /jobs/done
/2014/02/16/001216/job_1388830974669_1216162_conf.xml

通过上面的结果我们可以得到一下几点：

　　（1）、历史作业记录是存放在HDFS目录中；

　　（2）、由于历史作业记录可能非常多，所以历史作业记录是按照年/月/日的形式分别存放在相应的目录中，这样便于管理和查找；

　　（3）、对于每一个Hadoop历史作业记录相关信息都用两个文件存放，后缀名分别为*.jhist，*.xml。*.jhist文件里存放的是具体Hadoop作业的详细信息，如下：

{
 　　"type": "JOB_INITED",
 　　"event": {
    　　"org.apache.hadoop.mapreduce.jobhistory.JobInited": {
      　　 "jobid": "job_1388830974669_1215999",
      　　"launchTime": 1392477383583,
      　　 "totalMaps": 1,
      　　"totalReduces": 1,
      　　"jobStatus": "INITED",
      　　"uberized": false
    　　}
　　 }
}

这是Hadoop JOB初始化的一条信息，通过观察我们知道，*.jhist文件里面全部都是Json格式的数据。根据type进行区分这条Json的含义，在Hadoop中，总共包含了一下几个type：

"JOB_SUBMITTED",
"JOB_INITED",
"JOB_FINISHED",
"JOB_PRIORITY_CHANGED",
"JOB_STATUS_CHANGED",
"JOB_FAILED",
"JOB_KILLED",
"JOB_ERROR",
"JOB_INFO_CHANGED",
"TASK_STARTED",
"TASK_FINISHED",
"TASK_FAILED",
"TASK_UPDATED",
"NORMALIZED_RESOURCE",
"MAP_ATTEMPT_STARTED",
"MAP_ATTEMPT_FINISHED",
"MAP_ATTEMPT_FAILED",
"MAP_ATTEMPT_KILLED",
"REDUCE_ATTEMPT_STARTED",
"REDUCE_ATTEMPT_FINISHED",
"REDUCE_ATTEMPT_FAILED",
"REDUCE_ATTEMPT_KILLED",
"SETUP_ATTEMPT_STARTED",
"SETUP_ATTEMPT_FINISHED",
"SETUP_ATTEMPT_FAILED",
"SETUP_ATTEMPT_KILLED",
"CLEANUP_ATTEMPT_STARTED",
"CLEANUP_ATTEMPT_FINISHED",
"CLEANUP_ATTEMPT_FAILED",
"CLEANUP_ATTEMPT_KILLED",
"AM_STARTED"

而*.xml文件里面记录的是相应作业运行时候的完整参数配置，大家可以进去查看一下。

　　（4）、每一个作业的历史记录都存放在一个单独的文件中。

 　　mapreduce.jobhistory.intermediate-done-dir配置的目录下主要存放的是当前正在运行的Hadoop任务的记录相关信息，感兴趣的同学可以进去看看，这里就不介绍了。

 　　如果对Hadoop历史服务器WEB UI上提供的数据不满意，我们就可以通过对mapreduce.jobhistory.done-dir配置的目录进行分析，得到我们感兴趣的信息，比如统计某天中运行了多少个map、运行最长的作业用了多少时间、每个用户运行的Mapreduce任务数、总共运行了多少Mapreduce数等信息，这样对监控Hadoop集群是很好的，我们可以根据那些信息来确定怎么给某个用户分配资源等等。

细心的同学可能发现，在Hadoop历史服务器的WEB UI上最多显示20000个历史的作业记录信息；其实我们可以通过下面的参数进行配置，然后重启一下Hadoop jobhistory即可。

<property>
    <name>mapreduce.jobhistory.joblist.cache.size</name>
    <value>20000</value>
</property>